{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731b1e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 16 2025 11:58:17\n"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data,gymnasium ,time,math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504689be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(Currposition, endPosition): # [x,y,z]\n",
    "    distance_to_target = math.sqrt(math.pow(endPosition[0]-Currposition[0],2)+math.pow(endPosition[1]-Currposition[1],2)+math.pow(endPosition[2]-Currposition[2],2))\n",
    "    return distance_to_target\n",
    "\n",
    "class RexLeg:\n",
    "    def __init__(self, robot_id, leg_id, joints):\n",
    "        self.robot_id = robot_id\n",
    "        self.leg_id = leg_id\n",
    "        self.hip_joint, self.upper_joint, self.lower_joint = joints\n",
    "        self.joints = [self.hip_joint, self.upper_joint, self.lower_joint]\n",
    "\n",
    "    def set_pd_control(self, target_positions, kp=35, kd=1.0, max_force=45): #PD controller for some smoothing of motors\n",
    "        for joint, target_pos in zip(self.joints, target_positions):\n",
    "            pos, vel, _, _ = p.getJointState(self.robot_id, joint)\n",
    "            torque = kp * (target_pos - pos) - kd * vel\n",
    "            p.setJointMotorControl2(\n",
    "                bodyUniqueId=self.robot_id,\n",
    "                jointIndex=joint,\n",
    "                controlMode=p.TORQUE_CONTROL,\n",
    "                force=float(np.clip(torque, -max_force, max_force))\n",
    "            )\n",
    "class Rex:\n",
    "    def __init__(self, urdf_path, start_position,end_position):\n",
    "        self.robot_id = p.loadURDF(urdf_path, start_position)\n",
    "        self.num_joints = p.getNumJoints(self.robot_id)\n",
    "        self.legs = self.init_legs()\n",
    "        # Disable default motors set by urdf file \n",
    "        for j in range(self.num_joints):\n",
    "            p.setJointMotorControl2(\n",
    "                bodyIndex=self.robot_id,\n",
    "                jointIndex=j,\n",
    "                controlMode=p.VELOCITY_CONTROL,\n",
    "                force=0\n",
    "            )\n",
    "        self.start_pos  = start_position    # rex start position        (x,y,z)\n",
    "        self.end_pos    = end_position      # rexs desired end position (x,y,z)  \n",
    "\n",
    "    def init_legs(self):\n",
    "        leg_joint_map = {\n",
    "        0: [1, 2, 3],    # Front Right\n",
    "        1: [5, 6, 7],    # Front Left\n",
    "        2: [9, 10, 11],  # Rear Right\n",
    "        3: [13, 14, 15]  # Rear Left\n",
    "    }\n",
    "        return [RexLeg(self.robot_id, leg_id, indices)\n",
    "                for leg_id, indices in leg_joint_map.items()]\n",
    "\n",
    "    def get_observation(self):\n",
    "        obs = []\n",
    "        for leg in self.legs:\n",
    "            for joint in leg.joints:\n",
    "                pos, vel, _, _ = p.getJointState(self.robot_id, joint) # robots joints\n",
    "                obs.extend([pos, vel])\n",
    "\n",
    "        base_pos, base_orn = p.getBasePositionAndOrientation(self.robot_id) # base position (x,y,z)\n",
    "        base_lin_vel, base_ang_vel = p.getBaseVelocity(self.robot_id)       # base velocity (linear,angular (m/s))\n",
    "        obs.extend(base_pos)\n",
    "        obs.extend(base_lin_vel)\n",
    "        obs.extend(base_ang_vel)\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "\n",
    "class QuadrupedEnv(gymnasium.Env):\n",
    "    def __init__(self, render=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if render:\n",
    "            self.physics_client = p.connect(p.GUI)\n",
    "        else:\n",
    "            self.physics_client = p.connect(p.DIRECT) #set render false to disable gui\n",
    "\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        self.time_step = 1 / 240\n",
    "        p.setTimeStep(self.time_step)\n",
    "\n",
    "        self.max_episode_steps = 4000\n",
    "\n",
    "        self.plane_id = p.loadURDF(\"plane.urdf\")\n",
    "        self.rex = Rex(\"aliengo/aliengo.urdf\", [0, 0, 0.45],[5,5,0.45])\n",
    "        self.counter = 0\n",
    "\n",
    "        obs_dim = len(self.rex.get_observation())\n",
    "        self.observation_space = gymnasium.spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)\n",
    "\n",
    "        # actionspace is 12 joints 3 per leg (4th is fixed)\n",
    "        self.action_space = gymnasium.spaces.Box(low=-1.0, high=1.0, shape=(12,), dtype=np.float32)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        self.counter += 1\n",
    "        base_pose = [0.0, 0.4, -0.6] # default pose for each leg [hip, upper, lower]\n",
    "\n",
    "        # split action into 4 legs Ã— 3 joints\n",
    "        action = np.clip(action, -1.0, 1.0).reshape(4, 3)\n",
    "        # Scales for each joint so different joints can move different much\n",
    "        scales = [0.30, 0.60, 0.90]\n",
    "        for leg, act in zip(self.rex.legs, action):\n",
    "            target_positions = [bp + s * a for bp, s, a in zip(base_pose, scales, act)]\n",
    "            leg.set_pd_control(target_positions, kp=35, kd=1.0, max_force=50)\n",
    "\n",
    "        p.stepSimulation()\n",
    "\n",
    "        obs = self.rex.get_observation() # observartion\n",
    "        base_pos, base_orn = p.getBasePositionAndOrientation(self.rex.robot_id) #reward\n",
    "        base_lin_vel, base_ang_vel = p.getBaseVelocity(self.rex.robot_id)       # base velocity (linear,angular (m/s))\n",
    "        forward_vel = base_lin_vel[0]\n",
    "        distance_to_target = calculate_distance(base_pos,self.rex.end_pos)\n",
    "\n",
    "        # Reward = height stability + orientation uprightness\n",
    "        r_height = base_pos[2]\n",
    "        roll, pitch, yaw = p.getEulerFromQuaternion(base_orn) # roll = sideways, ptich = forward/backward, yaw = left/right\n",
    "        r_up_vector = p.getMatrixFromQuaternion(base_orn)[8]  # z-vector\n",
    "\n",
    "        # Speed rewards can't get less then 0\n",
    "        # and is capped at 4 so it won't try and cheat the system\n",
    "        v = max(0.0, forward_vel)\n",
    "        r_forward = np.tanh(v / 7.0)\n",
    "\n",
    "        # Award point for moving towards goal\n",
    "        progress = self.prev_distance - distance_to_target\n",
    "        progress_cap = 0.03\n",
    "        r_progress = np.clip(progress / progress_cap, -1.0, 1.0)\n",
    "\n",
    "        # Random rewards that didn't work.\n",
    "        r_upright = max(0, r_up_vector)\n",
    "        r_height_bonus = np.clip((r_height - 0.25) / 0.25, 0.0, 1.0)\n",
    "        r_pitch = 1 - abs(pitch)\n",
    "        r_roll = 1 -abs(roll)\n",
    "        r_stable = (r_pitch + r_roll) / 2\n",
    "\n",
    "        # Rewards that gives points for staying alive and having speed\n",
    "        r_survival = self.counter / self.max_episode_steps\n",
    "        r_movement = r_forward * r_survival\n",
    "        r_survival_speed = (self.counter / self.max_episode_steps) * r_forward\n",
    "\n",
    "        # Final rewards used in best model\n",
    "        reward = (\n",
    "            # rewards\n",
    "            1.6 * r_forward\n",
    "            + 1.2 * r_progress\n",
    "            + 0.5 * r_upright\n",
    "            # penalties\n",
    "            # small penality for many actions, to make it use less hopefully\n",
    "            -0.001 * np.sum(np.square(action))\n",
    "        )\n",
    "\n",
    "        info = {}\n",
    "        if r_height < 0.15 or abs(pitch) > 0.8 or abs(roll) > 0.8:\n",
    "            done = True\n",
    "            reward -= 5.0\n",
    "        if distance_to_target < 0.1:\n",
    "            done = True\n",
    "            reward+=100.0\n",
    "\n",
    "        self.prev_distance = distance_to_target\n",
    "        terminated = done\n",
    "        truncated = self.counter > self.max_episode_steps\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None):\n",
    "        p.resetSimulation()\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        p.setTimeStep(self.time_step)\n",
    "        self.plane_id = p.loadURDF(\"plane.urdf\")\n",
    "        self.rex = Rex(\"aliengo/aliengo.urdf\", [0, 0, 0.6],[5,5,0.45])\n",
    "        self.prev_distance = calculate_distance(self.rex.start_pos, self.rex.end_pos)\n",
    "        self.counter = 0\n",
    "\n",
    "        goal_vis = p.createVisualShape(\n",
    "            shapeType=p.GEOM_SPHERE,\n",
    "            radius=0.1,\n",
    "            rgbaColor=[1, 0, 0, 1]\n",
    "        )\n",
    "        goal_marker = p.createMultiBody(\n",
    "            baseVisualShapeIndex=goal_vis,\n",
    "            basePosition=self.rex.end_pos\n",
    "        )\n",
    "\n",
    "        info = {}\n",
    "        return self.rex.get_observation(), info\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect(self.physics_client) # stop rex forever :( NO RIP REX MY GUY MY G SAD RIP AMEN REST IN PEPERINO ((((\n",
    "\n",
    "def make_env(rank=0, seed=0):\n",
    "    def _init():\n",
    "        env = QuadrupedEnv(render=False)\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e11a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 66\n",
    "\n",
    "# Best model can be found as base_model.zip\n",
    "# Graphs for it can be found in logs\n",
    "\n",
    "# View or Training (True for view, False for Train)\n",
    "run_model = True\n",
    "# Starting training from base model? (True for base model, False for starting from nothing)\n",
    "train_from_base = False\n",
    "\n",
    "final_model_name = \"SAC_model\"\n",
    "load_model_name = \"base_model.zip\"\n",
    "view_model_name = \"base_model.zip\"\n",
    "\n",
    "time_step_training = 10_000_000\n",
    "learning_rate_sac = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cf35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Iris(R) Plus Graphics (ICL GT2)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 25.0.7-0ubuntu0.24.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 25.0.7-0ubuntu0.24.04.2\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Iris(R) Plus Graphics (ICL GT2)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X connection to :0 broken (explicit kill or server shutdown).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unknown parameter type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:368\u001b[39m, in \u001b[36mBasePolicy.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     actions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/stable_baselines3/sac/policies.py:353\u001b[39m, in \u001b[36mSACPolicy._predict\u001b[39m\u001b[34m(self, observation, deterministic)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> th.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/stable_baselines3/sac/policies.py:170\u001b[39m, in \u001b[36mActor.forward\u001b[39m\u001b[34m(self, obs, deterministic)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Note: the action is squashed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactions_from_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:190\u001b[39m, in \u001b[36mDiagGaussianDistribution.actions_from_params\u001b[39m\u001b[34m(self, mean_actions, log_std, deterministic)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mactions_from_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean_actions: th.Tensor, log_std: th.Tensor, deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> th.Tensor:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# Update the proba distribution\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_actions(deterministic=deterministic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:224\u001b[39m, in \u001b[36mSquashedDiagGaussianDistribution.proba_distribution\u001b[39m\u001b[34m(self, mean_actions, log_std)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mproba_distribution\u001b[39m(\n\u001b[32m    222\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfSquashedDiagGaussianDistribution, mean_actions: th.Tensor, log_std: th.Tensor\n\u001b[32m    223\u001b[39m ) -> SelfSquashedDiagGaussianDistribution:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:163\u001b[39m, in \u001b[36mDiagGaussianDistribution.proba_distribution\u001b[39m\u001b[34m(self, mean_actions, log_std)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[32m    158\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    161\u001b[39m \u001b[33;03m:return:\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m action_std = \u001b[43mth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m * log_std.exp()\n\u001b[32m    164\u001b[39m \u001b[38;5;28mself\u001b[39m.distribution = Normal(mean_actions, action_std)\n",
      "\u001b[31mRuntimeError\u001b[39m: unknown parameter type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         action, _ = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m         obs, reward, terminated, truncated, info = env.step(action)\n\u001b[32m     19\u001b[39m         done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:557\u001b[39m, in \u001b[36mBaseAlgorithm.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m    538\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    539\u001b[39m     observation: Union[np.ndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np.ndarray]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    542\u001b[39m     deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    543\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np.ndarray, ...]]]:\n\u001b[32m    544\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[33;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[33;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    555\u001b[39m \u001b[33;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:367\u001b[39m, in \u001b[36mBasePolicy.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    358\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    363\u001b[39m     )\n\u001b[32m    365\u001b[39m obs_tensor, vectorized_env = \u001b[38;5;28mself\u001b[39m.obs_to_tensor(observation)\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/torch/autograd/grad_mode.py:85\u001b[39m, in \u001b[36mno_grad.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprev\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Project/Project-Rex-Walking-Home/.venv/lib/python3.12/site-packages/torch/autograd/grad_mode.py:187\u001b[39m, in \u001b[36mset_grad_enabled.__init__\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mself\u001b[39m.prev = torch.is_grad_enabled()\n\u001b[32m    186\u001b[39m \u001b[38;5;28mself\u001b[39m.mode = mode\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_set_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: unknown parameter type"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "log_dir = \"./logs/\"\n",
    "checkpoint_dir =\"./checkpoints/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Runing the model that wanna be evalutated not training\n",
    "if run_model:\n",
    "    env = QuadrupedEnv(render=True)\n",
    "    model = SAC.load(view_model_name)\n",
    "    obs, _ = env.reset()\n",
    "    # Don't want it to close on max step wanna see until it fails\n",
    "    env.max_episode_steps = np.inf\n",
    "    done = False\n",
    "    # 5 runs just for fun\n",
    "    for ep in range(5):\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            #Lowering fps of the run so it doesn't zoom to fast.\n",
    "            time.sleep(1/120)\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "    env.close()\n",
    "else:\n",
    "    num_enviroments = 48 # code for making multiple enviroments \n",
    "    if (num_enviroments > 1 ):\n",
    "        env = DummyVecEnv([make_env(i, seed) for i in range(num_enviroments)])\n",
    "    else:   \n",
    "        env = QuadrupedEnv(render=True)\n",
    "\n",
    "    # Evaluation model that runs every 500k steps to see if the real model works as intended\n",
    "    eval_env = DummyVecEnv([make_env(888, seed)])\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=log_dir,\n",
    "        log_path=log_dir,\n",
    "        eval_freq=500_000 // num_enviroments,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    # Enable logging\n",
    "    logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "    # Setting up checkpoint to save very 500k so we can stop it under training and not lose progress.\n",
    "    # / with nr env so it get it right\n",
    "    save_freq = 500_000 // num_enviroments\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=save_freq,\n",
    "        save_path=\"./checkpoints/\",\n",
    "        name_prefix=\"SAC_rex\"\n",
    "        )\n",
    "\n",
    "    if train_from_base:\n",
    "        # Load model and comment out sac model if training from a base model.\n",
    "        sac_model = SAC.load(load_model_name, env=env)\n",
    "    else:\n",
    "        sac_model = SAC(\n",
    "            \"MlpPolicy\",\n",
    "            env,\n",
    "            verbose=0,\n",
    "            tensorboard_log=log_dir,\n",
    "            learning_rate=learning_rate_sac)\n",
    "\n",
    "    \n",
    "    # Setting logger, callback and starting training\n",
    "    sac_model.set_logger(logger)\n",
    "    callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "    sac_model.learn(total_timesteps=time_step_training, progress_bar=True, callback=callback)\n",
    "    # Saving model\n",
    "    sac_model.save(final_model_name)\n",
    "\n",
    "    del sac_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
